{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import requests\n",
    "import io\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "#import scheduler from torch.optim.lr_scheduler\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021cc5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    def __init__(self, n_qubits, n_layers, ansatz_type='basic', reuploading=False):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.ansatz_type = ansatz_type\n",
    "        self.reuploading = reuploading\n",
    "        \n",
    "        # Selezione device (Lightning è consigliato per velocità)\n",
    "        try:\n",
    "            self.dev = qml.device('lightning.qubit', wires=n_qubits)\n",
    "        except:\n",
    "            self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "            \n",
    "        @qml.qnode(self.dev, interface='torch')\n",
    "        def circuit(inputs, weights):\n",
    "            # Se Re-uploading è False (Paper standard), codifichiamo una volta sola all'inizio\n",
    "            if not self.reuploading:\n",
    "                for i in range(self.n_qubits):\n",
    "                    qml.RX(inputs[i], wires=i)\n",
    "            \n",
    "            # Loop sui layer\n",
    "            for layer in range(self.n_layers):\n",
    "                # DATA RE-UPLOADING (Solo per My QGRU)\n",
    "                # Reinseriamo l'input prima di ogni layer variazionale\n",
    "                if self.reuploading:\n",
    "                    for i in range(self.n_qubits):\n",
    "                        qml.RX(inputs[i], wires=i)\n",
    "                \n",
    "                # ANSATZ\n",
    "                if self.ansatz_type == 'basic':\n",
    "                    # --- Paper Style: Basic Entangler ---\n",
    "                    # Pesi shape: (n_layers, n_qubits) -> Qui usiamo weights[layer]\n",
    "                    for i in range(self.n_qubits):\n",
    "                        qml.RX(weights[layer, i], wires=i)\n",
    "                    # Entanglement ad anello\n",
    "                    for i in range(self.n_qubits - 1):\n",
    "                        qml.CNOT(wires=[i, i + 1])\n",
    "                    if self.n_qubits > 1:\n",
    "                        qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "                        \n",
    "                elif self.ansatz_type == 'strong':\n",
    "                    # --- My QGRU Style: Strongly Entangling ---\n",
    "                    # Pesi shape: (n_layers, n_qubits, 3) -> Passiamo la slice del layer\n",
    "                    # StronglyEntanglingLayers si aspetta (1, n_qubits, 3) per layer singolo\n",
    "                    # quindi passiamo weights[layer] espanso\n",
    "                    w_layer = weights[layer].unsqueeze(0) \n",
    "                    qml.StronglyEntanglingLayers(w_layer, wires=range(self.n_qubits))\n",
    "            \n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "        \n",
    "        self.circuit = circuit\n",
    "    \n",
    "    def forward(self, inputs, weights):\n",
    "        batch_size = inputs.shape[0]\n",
    "        results = []\n",
    "        for i in range(batch_size):\n",
    "            res = self.circuit(inputs[i], weights)\n",
    "            if isinstance(res, list):\n",
    "                res = torch.stack(res)\n",
    "            results.append(res)\n",
    "        return torch.stack(results).float()\n",
    "    \n",
    "class VQCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_qubits, n_layers, \n",
    "                 use_scale_factor=False, ansatz_type='basic', reuploading=False):\n",
    "        super(VQCLayer, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.use_scale_factor = use_scale_factor\n",
    "        \n",
    "        self.fc_in = nn.Linear(input_dim, n_qubits)\n",
    "        \n",
    "        # Inizializza circuito\n",
    "        self.qc = QuantumCircuit(n_qubits, n_layers, ansatz_type, reuploading)\n",
    "        \n",
    "        # Gestione parametri in base all'Ansatz\n",
    "        if ansatz_type == 'strong':\n",
    "            # StronglyEntangling vuole 3 parametri per qubit (Rotazione X, Y, Z)\n",
    "            self.q_params = nn.Parameter(torch.randn(n_layers, n_qubits, 3) * 0.1)\n",
    "        else:\n",
    "            # BasicEntangler vuole 1 parametro per qubit (Rotazione X)\n",
    "            self.q_params = nn.Parameter(torch.randn(n_layers, n_qubits) * 0.1)\n",
    "            \n",
    "        self.fc_out = nn.Linear(n_qubits, hidden_dim)\n",
    "\n",
    "        # Scale Factor Vettoriale (Uno per qubit) se richiesto\n",
    "        if self.use_scale_factor:\n",
    "            self.scale_factor = nn.Parameter(torch.ones(n_qubits))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        \n",
    "        if self.use_scale_factor:\n",
    "            # My QGRU: Tanh + Scala Vettoriale\n",
    "            x = torch.tanh(x) * self.scale_factor \n",
    "        else:\n",
    "            # Paper QGRU: Identity (lascia decidere al layer lineare)\n",
    "            pass \n",
    "        \n",
    "        x = self.qc.forward(x, self.q_params)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_qubits, n_layers, \n",
    "                 use_scale_factor, ansatz_type, reuploading):\n",
    "        super(QGRUCell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        concat_dim = hidden_dim + input_dim\n",
    "        \n",
    "        # Configurazione condivisa per i 3 gate\n",
    "        config = {\n",
    "            'input_dim': concat_dim, 'hidden_dim': hidden_dim,\n",
    "            'n_qubits': n_qubits, 'n_layers': n_layers,\n",
    "            'use_scale_factor': use_scale_factor,\n",
    "            'ansatz_type': ansatz_type, 'reuploading': reuploading\n",
    "        }\n",
    "        \n",
    "        self.vqc_reset = VQCLayer(**config)\n",
    "        self.vqc_update = VQCLayer(**config)\n",
    "        self.vqc_output = VQCLayer(**config)\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        combined = torch.cat([h_prev, x], dim=1)\n",
    "        \n",
    "        r_t = torch.sigmoid(self.vqc_reset(combined))\n",
    "        z_t = torch.sigmoid(self.vqc_update(combined))\n",
    "        \n",
    "        combined_reset = torch.cat([r_t * h_prev, x], dim=1)\n",
    "        h_tilde = torch.tanh(self.vqc_output(combined_reset))\n",
    "        \n",
    "        h = z_t * h_prev + (1 - z_t) * h_tilde\n",
    "        return h\n",
    "class QGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_qubits, n_layers, \n",
    "                 use_scale_factor=False, ansatz_type='basic', reuploading=False):\n",
    "        super(QGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.qgru_cell = QGRUCell(input_dim, hidden_dim, n_qubits, n_layers, \n",
    "                                  use_scale_factor, ansatz_type, reuploading)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, h_0=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        if h_0 is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_dim, device=x.device)\n",
    "        else:\n",
    "            h = h_0\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            h = self.qgru_cell(x[:, t, :], h)\n",
    "            out = self.fc_out(h)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs, h\n",
    "    \n",
    "\n",
    "\n",
    "class ClassicalGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ClassicalGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, h = self.gru(x)\n",
    "        out = self.fc_out(out)\n",
    "        return out, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2deb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, window_size):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.window_size = window_size\n",
    "    def __len__(self): return len(self.data) - self.window_size\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx:idx+self.window_size, :-1], self.data[idx+1:idx+self.window_size+1, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    try:\n",
    "        url = \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\"\n",
    "        print(\"Scaricamento dataset ETTh1...\")\n",
    "        df = pd.read_csv(io.StringIO(requests.get(url, timeout=10).content.decode('utf-8')))\n",
    "        return df[['HUFL', 'HULL', 'MUFL', 'OT']].values\n",
    "    except:\n",
    "        print(\"Fallback su dati sintetici...\")\n",
    "        t = np.linspace(0, 100, 2000)\n",
    "        return np.column_stack([np.sin(t), np.cos(t), np.sin(t)*np.cos(t), np.sin(t+0.5)])\n",
    "\n",
    "def evaluate_final(model, loader, device):\n",
    "    model.eval()\n",
    "    mse_loss, mae_loss = 0, 0\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_mae = nn.L1Loss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out, _ = model(x)\n",
    "            mse_loss += criterion_mse(out, y).item()\n",
    "            mae_loss += criterion_mae(out, y).item()\n",
    "    return mse_loss / len(loader), mae_loss / len(loader)\n",
    "\n",
    "def train_experiment(model, train_loader, val_loader, epochs, lr, device, name):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    # Scheduler come da paper (opzionale se poche epoche, ma corretto averlo)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    start_total = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in tqdm(train_loader, desc=f\"Ep {epoch+1}/{epochs}\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out, _ = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_train = train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                out, _ = model(x.to(device))\n",
    "                val_loss += criterion(out, y.to(device)).item()\n",
    "        avg_val = val_loss / len(val_loader)\n",
    "        \n",
    "        history['train_loss'].append(avg_train)\n",
    "        history['val_loss'].append(avg_val)\n",
    "        \n",
    "        print(f\"Ep {epoch+1} | Train: {avg_train:.5f} | Val: {avg_val:.5f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"Best Val Loss: {best_val_loss:.5f}\")\n",
    "    \n",
    "    total_time = time.time()-start_total\n",
    "    print(f\"Total Time {name}: {total_time:.1f}s\")\n",
    "    return history, total_time\n",
    "\n",
    "def get_model_summary(model, name):\n",
    "    print(f\"\\n{'='*10} {name} SUMMARY {'='*10}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Conta parametri quantistici (quelli chiamati 'q_params')\n",
    "    q_params = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'q_params' in n:\n",
    "            q_params += p.numel()\n",
    "    \n",
    "    c_params = total_params - q_params\n",
    "    \n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"  - Classical: {c_params}\")\n",
    "    print(f\"  - Quantum:   {q_params}\")\n",
    "    print(f\"{'='*30}\")\n",
    "\n",
    "def draw_model_circuit(model):\n",
    "    # Verifica se è un modello QGRU controllando se ha l'attributo 'qgru_cell'\n",
    "    if not hasattr(model, 'qgru_cell'):\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Quantum Circuit Visualization (Reset Gate) ---\")\n",
    "    # Estraiamo il circuito dal reset gate\n",
    "    try:\n",
    "        vqc = model.qgru_cell.vqc_reset\n",
    "        qc_instance = vqc.qc\n",
    "        \n",
    "        # Creiamo input dummy\n",
    "        n_qubits = qc_instance.n_qubits\n",
    "        dummy_inputs = torch.rand(n_qubits)\n",
    "        \n",
    "        # Creiamo pesi dummy con la shape corretta\n",
    "        # VQCLayer inizializza q_params con shape (n_layers, n_qubits) o (n_layers, n_qubits, 3)\n",
    "        # Possiamo prendere direttamente i pesi dal modello per coerenza, ma staccati dal grafo\n",
    "        dummy_weights = vqc.q_params.detach().clone().cpu()\n",
    "        \n",
    "        # Disegniamo\n",
    "        drawer = qml.draw(qc_instance.circuit)\n",
    "        print(drawer(dummy_inputs, dummy_weights))\n",
    "    except Exception as e:\n",
    "        print(f\"Impossibile disegnare il circuito: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a00526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Scaricamento dataset ETTh1...\n",
      "\n",
      "[1/3] Avvio My QGRU (StronglyEntangling + Re-uploading)...\n",
      "\n",
      "--- Training MY QGRU ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/10:   0%|          | 1/218 [01:40<6:02:34, 100.25s/it]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cpu\") # CPU consigliata per simulazioni piccoli circuiti\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # --- IPERPARAMETRI (Specifiche Paper) ---\n",
    "    WINDOW_SIZE = 5\n",
    "    HIDDEN_DIM = 5  \n",
    "    N_QUBITS = 5    \n",
    "    N_LAYERS = 2    \n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 10      # Aumenta per risultati migliori (paper usa >100)\n",
    "    LR = 0.005\n",
    "\n",
    "    # Dati\n",
    "    raw = load_data()\n",
    "    split_idx = int(len(raw) * 0.8)\n",
    "    \n",
    "    scaler = MinMaxScaler((-1, 1)).fit(raw[:split_idx])\n",
    "    data_scaled = scaler.transform(raw)\n",
    "    \n",
    "    train_ds = TimeSeriesDataset(data_scaled[:split_idx], WINDOW_SIZE)\n",
    "    val_ds = TimeSeriesDataset(data_scaled[split_idx:], WINDOW_SIZE)\n",
    "    \n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    dims = (raw.shape[1]-1, HIDDEN_DIM, 1) # (Input, Hidden, Output)\n",
    "    results_list = []\n",
    "\n",
    "    # --- 1. MY QGRU (Potenziata: Strong Ent. + Re-upload + Scaling) ---\n",
    "    print(\"\\n[1/3] Avvio My QGRU (StronglyEntangling + Re-uploading)...\")\n",
    "    my_qgru = QGRU(*dims, N_QUBITS, N_LAYERS, \n",
    "                   use_scale_factor=True, \n",
    "                   ansatz_type='strong', \n",
    "                   reuploading=True)\n",
    "    \n",
    "    get_model_summary(my_qgru, \"My QGRU\")\n",
    "    draw_model_circuit(my_qgru)\n",
    "    \n",
    "    hist_my, time_my = train_experiment(my_qgru, train_dl, val_dl, EPOCHS, LR, device, \"MY QGRU\")\n",
    "    mse_my, mae_my = evaluate_final(my_qgru, val_dl, device)\n",
    "    \n",
    "    results_list.append({\n",
    "        \"Model\": \"My QGRU (Optimized)\",\n",
    "        \"Test MSE\": mse_my, \"Test MAE\": mae_my, \"Time (s)\": time_my\n",
    "    })\n",
    "\n",
    "    # --- 2. PAPER QGRU (Basic Ent. + No Re-upload + No Scaling) ---\n",
    "    print(\"\\n[2/3] Avvio Paper QGRU (BasicEntangler Standard)...\")\n",
    "    paper_qgru = QGRU(*dims, N_QUBITS, N_LAYERS, \n",
    "                      use_scale_factor=False, \n",
    "                      ansatz_type='basic', \n",
    "                      reuploading=False)\n",
    "    \n",
    "    get_model_summary(paper_qgru, \"Paper QGRU\")\n",
    "    draw_model_circuit(paper_qgru)\n",
    "    \n",
    "    hist_paper, time_paper = train_experiment(paper_qgru, train_dl, val_dl, EPOCHS, LR, device, \"PAPER QGRU\")\n",
    "    mse_paper, mae_paper = evaluate_final(paper_qgru, val_dl, device)\n",
    "    \n",
    "    results_list.append({\n",
    "        \"Model\": \"Paper QGRU\",\n",
    "        \"Test MSE\": mse_paper, \"Test MAE\": mae_paper, \"Time (s)\": time_paper\n",
    "    })\n",
    "    \n",
    "    # --- 3. CLASSICAL GRU ---\n",
    "    print(\"\\n[3/3] Avvio Classical GRU...\")\n",
    "    classic_gru = ClassicalGRU(*dims)\n",
    "    \n",
    "    get_model_summary(classic_gru, \"Classical GRU\")\n",
    "    \n",
    "    hist_classic, time_classic = train_experiment(classic_gru, train_dl, val_dl, EPOCHS, LR, device, \"CLASSICAL GRU\")\n",
    "    mse_classic, mae_classic = evaluate_final(classic_gru, val_dl, device)\n",
    "    \n",
    "    results_list.append({\n",
    "        \"Model\": \"Classical GRU\",\n",
    "        \"Test MSE\": mse_classic, \"Test MAE\": mae_classic, \"Time (s)\": time_classic\n",
    "    })\n",
    "    \n",
    "    # --- SALVATAGGIO CSV ---\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    df_results.to_csv(\"risultati_finali_reupload.csv\", index=False)\n",
    "    print(\"\\n✅ Risultati salvati in 'risultati_finali_reupload.csv'\")\n",
    "    print(df_results)\n",
    "    \n",
    "    # --- PLOT ---\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Train Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist_my['train_loss'], label='My QGRU (Strong+ReUp)', marker='o', color='blue')\n",
    "    plt.plot(hist_paper['train_loss'], label='Paper QGRU (Basic)', marker='s', linestyle='--', color='green')\n",
    "    plt.plot(hist_classic['train_loss'], label='Classic GRU', marker='^', linestyle=':', color='red')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Val Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(hist_my['val_loss'], label='My QGRU', marker='o', color='blue')\n",
    "    plt.plot(hist_paper['val_loss'], label='Paper QGRU', marker='s', linestyle='--', color='green')\n",
    "    plt.plot(hist_classic['val_loss'], label='Classic GRU', marker='^', linestyle=':', color='red')\n",
    "    plt.title(\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparison_reupload_strong.png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantumcomputing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
